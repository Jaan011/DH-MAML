"""
2D navigation problems, as described in [1]. in MAML paper.

The code is adapted from
https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py

In this problem  at each time step,
    * The agent takes an action (its velocity, clipped in [-0.1, 0.1]),
     It receives a penalty equal to its L2 distance to the goal position
     (the reward is `-distance`).

   The 2D navigation tasks are generated by sampling goal positions
   from the uniform distribution on [-0.5, 0.5]^2.

[1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
    Meta-Learning for Fast Adaptation of Deep Networks", 2017
    (https://arxiv.org/abs/1703.03400)

Mus
"""
from __future__ import annotations

from abc import ABC
from typing import Tuple, Optional

import numpy as np
import gym

from gym import spaces
from gym.core import ObsType
import torch

from meta_critics.envs.env_types import EnvType


class Navigation(gym.Env, ABC):

    def __init__(self, task=None,
                 low: Optional[float] = -0.5,
                 high: Optional[float] = 0.5,
                 out: Optional[EnvType] = EnvType.NdArray):
        """
        :param task:
        :param low:
        :param high:
        """
        super(Navigation, self).__init__()
        self.out = None

        if task is None:
            task = {}

        self.low = low
        self.high = high

        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)
        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(2,), dtype=np.float32)
        self._goal = task.get('goal', np.zeros(2, dtype=np.float32))
        self._state = np.zeros(2, dtype=np.float32)
        self._task = task

    def sample(self) -> np.ndarray | torch.Tensor:
        """ Sampling from action space.
        :returns: Random action from action space return either ndarray or tensor
        """
        if self.out == EnvType.Tensor:
            return torch.from_numpy(self.action_space.sample())
        return self.action_space.sample()

    def sample_tasks(self, num_tasks):
        """
        :param num_tasks:
        :return:
        """
        goals = self.np_random.uniform(self.low, self.high, size=(num_tasks, 2))
        tasks = [{'goal': goal} for goal in goals]
        return tasks

    def obs_shape(self):
        """Return shape of observation
        :return:
        """
        if isinstance(self.observation_space, gym.spaces.Discrete):
            obs_shape = (1,)
        elif isinstance(self.observation_space, gym.spaces.Box):
            obs_shape = self.observation_space.shape
        else:
            raise ValueError("Unsupported observation space")
        return obs_shape

    def action_shape(self):
        """Return shape of action.
        :return:
        """
        if isinstance(self.action_space, gym.spaces.Box):
            action_shape = self.action_space.shape
        elif isinstance(self.action_space, gym.spaces.Discrete):
            action_shape = (1,)
        else:
            raise ValueError("Unsupported action space")
        return action_shape

    def reset_task(self, task):
        """
        :param task:
        :return:
        """
        self._task = task
        self._goal = task['goal']

    def reset(self, *,
              seed: Optional[int] = None,
              options: Optional[dict] = None) -> Tuple[ObsType, dict]:
        """
        :param seed:
        :param options:
        :return:
        """
        super().reset(seed=seed)
        if self.render_mode == "human":
            self.render()

        # self._goal = np.random.uniform(self.low, self.high, size=(2,))
        self._state = np.zeros(2, dtype=np.float32)
        return self._state, {'task': self._task}

    def step(self, action) -> Tuple[ObsType, float, bool, bool, dict]:
        """
        :param action:
        :return:
        """
        action = np.clip(action, -0.1, 0.1)
        assert self.action_space.contains(action)
        self._state = self._state + action
        x = self._state[0] - self._goal[0]
        y = self._state[1] - self._goal[1]
        return (self._state, -np.sqrt(x ** 2 + y ** 2),
                ((np.abs(x) < 0.01) and (np.abs(y) < 0.01)),
                False, {'task': self._task})
