"""Multi-armed bandit problems with Bernoulli observations, as described
   in [1].

   At each time step, the agent pulls one of the `k` possible arms (actions),
   say `i`, and receives a reward sampled from a Bernoulli distribution with
   parameter `p_i`. The multi-armed bandit tasks are generated by sampling
   the parameters `p_i` from the uniform distribution on [0, 1].

   [1] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever,
       Pieter Abbeel, "RL2: Fast Reinforcement Learning via Slow Reinforcement
       Learning", 2016 (https://arxiv.org/abs/1611.02779)
   """
from abc import ABC
from typing import Tuple

import numpy as np
from gym.core import ObsType
from gym.vector.utils import spaces

from meta_critics.envs.bandits.bandit_base_env import BanditEnv


class GaussianBanditEnv(BanditEnv, ABC):
    """Multi-armed problems with Gaussian observations.

    At each time step, the agent pulls one of the `k` possible arms (actions),
    say `i`, and receives a reward sampled from a Normal distribution with
    mean `p_i` and standard deviation `std` (fixed across all tasks). The
    multi-armed bandit tasks are generated by sampling the parameters `p_i`
    from the uniform distribution on [0, 1].
    """

    def __init__(self, k, std=1.0, task=None):
        """
        :param k:
        :param std:
        :param task:
        """
        super(GaussianBanditEnv, self).__init__(k=k, task=task)
        assert self.k > 0
        assert self.max_reward() > 0

        self.std = std
        self.action_space = spaces.Discrete(self.k)
        self.observation_space = spaces.Box(low=0, high=0, shape=(1,), dtype=np.float32)
        self.task = task

        print(f"Creating GaussianBanditEnv task {task}, k={k}")

        if task is None:
            task = {}
        self._task = task

        self._means = task.get('mean', np.full((k,), 0.5, dtype=np.float32))

    def sample_tasks(self, num_tasks):
        """
        :param num_tasks:
        :return:
        """
        means = self.np_random.rand(num_tasks, self.k)
        tasks = [{'mean': mean} for mean in means]
        return tasks

    def reset_task(self, task):
        """
        :param task:
        :return:
        """
        self._task = task
        self._means = task['mean']

    def step(self, action) -> Tuple[ObsType, float, bool, bool, dict]:
        """
        :param action:
        :return:
        """
        assert self.action_space.contains(action)
        mean = self._means[action]
        reward = self.np_random.normal(mean, self.std)
        observation = np.zeros(1, dtype=np.float32)
        return observation, reward, True, True, {'task': self._task}