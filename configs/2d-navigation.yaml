experiment_name: "navigation"
env_name: "navigation-v0"

# this you wandb project and username so you can get all stats.
wandb:
  entity: spyroot
  project: dh-maml

model_root: "models"  # a root dir so all folder will be under.
log_dir: "~/meta_critics/logs/navigation"
model_dir: "~/meta_critics/models/navigation"
create_dir: True # will create all dir

env-args:
  low: -0.5
  high: 0.5

# if we need rename dtypes. It important if model see a float64
# and it doesn't know about that you will hit all sort of issues
trajectory_sampler:
  remap_types: False
#  reward_dtype: torch.float64
#  action_dtype: torch.float32
#  observations_dtype: torch.float32

trainer:
  use_wandb: True
  type: distributed_rpc
  use_discount_gamma: True
  gae_lambda_factor: 1.0
  gamma_factor: 0.99
  meta_test_freq: 100
  save_freq: 20
  use_gae: True
  resume: True
  num-workers: 1

# Policy network
policy_network:
  hidden_sizes: [64, 64]
  activation: "tanh"

meta_task:
  num_meta_test: 10
  num_batches: 500       # outer-loop updates
  num_meta_task: 40      # number of task
  num_trajectory: 20     # Number of trajectories to sample for each task.
  num_steps: 1           # Number of gradient steps in the inner loop / fast adaptation.
  fast_lr: 0.1           # Step size for each gradient step in the inner loop / fast adaptation.
  first_order: false

model:
  name: trpo
  model_type: ConcurrentMamlTRPO  # model type.
  max_kl: 1.0e-2             # size of the trust-region
  cg_iters: 10               # Number of iterations of Conjugate Gradient.
  cg_damping: 1.0e-5         # Value of the damping in Conjugate Gradient.
  ls_max_steps: 15           # Maximum number of steps in the line search.
  ls_backtrack_ratio: 0.8    # Ratio to use for